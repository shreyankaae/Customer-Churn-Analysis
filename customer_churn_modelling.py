# -*- coding: utf-8 -*-
"""customer_churn_modelling.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18WtCKBtP5YzKm193gafZZZ6W0PvcZC9p
"""

#libraries
import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.metrics import roc_auc_score, roc_curve
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestClassifier
import xgboost as xgb
import lightgbm as lgb

df = pd.read_csv('/content/cleaned_telco (2).csv')
print(df.shape)
df.head()

print(df['Churn'].value_counts())

X = df.drop('Churn', axis=1)
y = df['Churn']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

cat_cols = X.select_dtypes(include='object').columns

# One-hot encoding
X_encoded = pd.get_dummies(X, columns=cat_cols, drop_first=True)

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_encoded)

X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.3, random_state=42, stratify=y)

lr_model = LogisticRegression(max_iter=1000)
lr_model.fit(X_train, y_train)

y_pred = lr_model.predict(X_test)
print(classification_report(y_test, y_pred))

from sklearn.metrics import accuracy_score, confusion_matrix

print("Accuracy:", accuracy_score(y_test, y_pred))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))

#random forest classifier
model = RandomForestClassifier(random_state=42)
model.fit(X_train, y_train)

import xgboost as xgb
xgb_model = xgb.XGBClassifier(random_state=42)
xgb_model.fit(X_train, y_train)

import lightgbm as lgb
lgb_model = lgb.LGBMClassifier(random_state=42)
lgb_model.fit(X_train, y_train)

# Telco Customer Churn - Hyperparameter Tuning with XGBoost

import pandas as pd
import numpy as np

# ML libraries
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score
from xgboost import XGBClassifier

# Convert TotalCharges to numeric
df["total_charges"] = pd.to_numeric(df["total_charges"], errors='coerce')
df["total_charges"] = df["total_charges"].fillna(df["total_charges"].median())

# Encode target
df["Churn"] = df["Churn"].map({"Yes":1, "No":0})

# One-hot encode categorical variables
df = pd.get_dummies(df, drop_first=True)

# Split features/target
X = df.drop("Churn", axis=1)
y = df["churn_value"].astype(int)

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# XGBoost model
xgb = XGBClassifier(eval_metric="logloss", use_label_encoder=False)

# Hyperparameter grid
param_dist = {
    "n_estimators": [100, 200, 300, 400, 500],
    "max_depth": [3, 4, 5, 6, 8, 10],
    "learning_rate": [0.01, 0.05, 0.1, 0.2],
    "subsample": [0.6, 0.8, 1.0],
    "colsample_bytree": [0.6, 0.8, 1.0],
    "gamma": [0, 1, 5],
    "min_child_weight": [1, 3, 5]
}

# Randomized Search
random_search = RandomizedSearchCV(
    estimator=xgb,
    param_distributions=param_dist,
    n_iter=30,   # try 30 combinations
    scoring="roc_auc",  # focus on churn prediction
    cv=5,
    verbose=2,
    random_state=42,
    n_jobs=-1
)

# Fit
random_search.fit(X_train, y_train)

# Best parameters
print("Best Parameters:", random_search.best_params_)

# Evaluate on test set
best_model = random_search.best_estimator_
y_pred = best_model.predict(X_test)
y_prob = best_model.predict_proba(X_test)[:,1]

print("\nClassification Report:\n", classification_report(y_test, y_pred))
print("ROC-AUC Score:", roc_auc_score(y_test, y_prob))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))

# Encode churn as binary for checking correlation
df["churn_flag"] = df["Churn"].map({"Yes": 1, "No": 0})

# Calculate correlation (only for numeric features)
corr = df.corr(numeric_only=True)["churn_flag"].sort_values(ascending=False)
print(corr)

# Select only numeric columns
numeric_df = df.select_dtypes(include=["int64", "float64"])

# Correlation of churn_flag with numeric features
leakage_corr = numeric_df.corr()["churn_flag"].sort_values(ascending=False)
print(leakage_corr)

# Leakage columns to drop
leakage_cols = [
    "churn_value",       # direct churn indicator
    "churn_score",       # churn risk score (post-calculated)
    "cltv",              # customer lifetime value (post-event)
    "churn_reason",      # categorical churn reason
    "churn_category",    # grouped churn reason
]

# Also drop churn_reason_* dummy columns if they exist
leakage_reason_cols = [col for col in df.columns if col.startswith("churn_reason_")]

# Combine
drop_cols = leakage_cols + leakage_reason_cols

# Drop from dataframe
df = df.drop(columns=drop_cols, errors="ignore")

print(f"Dropped {len(drop_cols)} leakage columns.")

# 1. Define target column
target = "churn_flag"

# 2. Remove leakage columns
leakage_cols = [
    "churn_value",
    "churn_score",
    "cltv",
    "churn_reason",
    "churn_category",
]

# Drop any churn_reason_* dummy columns too
leakage_reason_cols = [col for col in df.columns if col.startswith("churn_reason_")]

drop_cols = leakage_cols + leakage_reason_cols

df = df.drop(columns=drop_cols, errors="ignore")

# 3. Split into features (X) and target (y)
X = df.drop(columns=[target])
y = df[target]

print("Features shape:", X.shape)
print("Target shape:", y.shape)
print("Remaining feature columns:", X.columns.tolist()[:20], "...")

import shap

# Create explainer
explainer = shap.TreeExplainer(best_model)
shap_values = explainer.shap_values(X_test)

# Global feature importance
shap.summary_plot(shap_values, X_test, plot_type="bar")

# Detailed summary (beeswarm plot)
shap.summary_plot(shap_values, X_test)

# Example: explain a single customer
shap.force_plot(explainer.expected_value, shap_values[0,:], X_test.iloc[0,:])

import joblib

# Save cleaned dataset
X_test.to_csv("cleaned_telco_features.csv", index=False)
y_test.to_csv("cleaned_telco_labels.csv", index=False)

# Save trained model
joblib.dump(best_model, "churn_xgb_model.pkl")

print(df.columns.tolist())

# Load the raw dataset again
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

raw_df = pd.read_csv("/content/cleaned_telco (2).csv")

# Check original churn values
print("Raw Churn counts:")
print(raw_df["Churn"].value_counts())

# Plot churn by contract type
plt.figure(figsize=(8, 5))
sns.countplot(x="contract", hue="Churn", data=raw_df)
plt.title("Churn by Contract Type")
plt.xlabel("Contract Type")
plt.ylabel("Number of Customers")
plt.show()

!pip install streamlit pyngrok -q

!ngrok config add-authtoken YOUR_AUTHTOKEN_HERE

import joblib
joblib.dump(X_train.columns.tolist(), "features.pkl")

